{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":99552,"databundleVersionId":13441085,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport shutil\nimport torch\nimport numpy as np\nimport SimpleITK as sitk\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.notebook import tqdm\n\n# ==============================================================================\n# 1. CONFIGURATION & AUTOMATIC PATH DISCOVERY\n# ==============================================================================\n# Define the root directory where all the series folders are located.\nSERIES_ROOT_DIR = '/kaggle/input/rsna-intracranial-aneurysm-detection/series'\n\n# The OUTPUT_DIR you specified\nOUTPUT_DIR = \"/kaggle/working/resampled_nifti\"\n\n# --- AUTOMATICALLY FIND ALL SERIES DIRECTORIES ---\n# The script will now scan the SERIES_ROOT_DIR and create the list of paths to process.\n# This replaces the need for you to create a list manually.\nprint(f\"Scanning for series directories in: {SERIES_ROOT_DIR}...\")\nbatch_series_paths = sorted([\n    os.path.join(SERIES_ROOT_DIR, name)\n    for name in os.listdir(SERIES_ROOT_DIR)\n    if os.path.isdir(os.path.join(SERIES_ROOT_DIR, name))\n])\n\nif not batch_series_paths:\n    raise FileNotFoundError(f\"No series directories found in {SERIES_ROOT_DIR}. Please check the path.\")\n\nprint(f\"Found {len(batch_series_paths)} unique series to process.\")\n\n\n# ==============================================================================\n# 2. THE DATASET CLASS (No changes needed here)\n# ==============================================================================\n\nclass DicomSeriesDataset(Dataset):\n    \"\"\"\n    This Dataset class is already correctly designed to work with a list of\n    directory paths. It checks for pre-processed NIfTI files to allow for\n    restarting a failed run.\n    \"\"\"\n    def __init__(self, series_paths, new_spacing=(1, 1, 1), transform=None, save_dir=None):\n        self.series_paths = series_paths\n        self.new_spacing = new_spacing\n        self.transform = transform\n        self.save_dir = save_dir\n        if save_dir:\n            os.makedirs(save_dir, exist_ok=True)\n\n    def __len__(self):\n        return len(self.series_paths)\n\n    def __getitem__(self, idx):\n        series_path = self.series_paths[idx]\n        series_name = os.path.basename(os.path.normpath(series_path))\n        \n        if self.save_dir:\n            save_path = os.path.join(self.save_dir, f\"{series_name}.nii.gz\")\n            if os.path.exists(save_path):\n                resampled_img = sitk.ReadImage(save_path)\n            else:\n                resampled_img = self._load_and_resample_dicom(series_path, save_path)\n        else:\n            resampled_img = self._load_and_resample_dicom(series_path)\n\n        arr = sitk.GetArrayFromImage(resampled_img).astype(np.float32)\n        arr = np.expand_dims(arr, axis=0)\n        tensor = torch.from_numpy(arr)\n\n        if self.transform:\n            tensor = self.transform(tensor)\n\n        return tensor, series_name\n\n    def _load_and_resample_dicom(self, series_path, save_path=None):\n        try:\n            reader = sitk.ImageSeriesReader()\n            dicom_names = reader.GetGDCMSeriesFileNames(series_path)\n            if not dicom_names:\n                print(f\"Warning: No DICOM files found in {series_path}\")\n                return sitk.Image(1, 1, 1, sitk.sitkFloat32)\n            reader.SetFileNames(dicom_names)\n            sitk_img = reader.Execute()\n\n            if sitk_img.GetDimension() == 4:\n                size = list(sitk_img.GetSize())\n                size[3] = 0; index = [0, 0, 0, 0]\n                sitk_img = sitk.Extract(sitk_img, size, index)\n\n            sitk_img = sitk.Cast(sitk_img, sitk.sitkFloat32)\n            original_spacing = sitk_img.GetSpacing()\n            original_size = sitk_img.GetSize()\n            new_size = [int(round(osz * ospc / nspc)) for osz, ospc, nspc in zip(original_size, original_spacing, self.new_spacing)]\n\n            resample_filter = sitk.ResampleImageFilter()\n            resample_filter.SetDefaultPixelValue(0)\n            resample_filter.SetOutputSpacing(self.new_spacing)\n            resample_filter.SetSize(new_size)\n            resample_filter.SetOutputDirection(sitk_img.GetDirection())\n            resample_filter.SetOutputOrigin(sitk_img.GetOrigin())\n            resample_filter.SetInterpolator(sitk.sitkLinear)\n            resampled_img = resample_filter.Execute(sitk_img)\n\n            if save_path:\n                sitk.WriteImage(resampled_img, save_path)\n                \n            return resampled_img\n        except Exception as e:\n            print(f\"Error processing {series_path}: {e}\")\n            return sitk.Image(1, 1, 1, sitk.sitkFloat32)\n\n\n# ==============================================================================\n# 3. MAIN PROCESSING SCRIPT WITH CHUNKING LOGIC\n# ==============================================================================\n\n# --- CONFIGURATION ---\nCHUNK_SIZE = 100\nTOTAL_FILES = len(batch_series_paths)\nNUM_CHUNKS = -(-TOTAL_FILES // CHUNK_SIZE) # Ceiling division\n\nprint(f\"Total unique series to process: {TOTAL_FILES}\")\nprint(f\"Processing in {NUM_CHUNKS} chunks of up to {CHUNK_SIZE} series each.\")\nprint(\"-\" * 50)\n\n\n# --- MAIN LOOP ---\nfor i in range(NUM_CHUNKS):\n    start_index = i * CHUNK_SIZE\n    end_index = min((i + 1) * CHUNK_SIZE, TOTAL_FILES)\n    \n    print(f\"\\n--- Processing Chunk {i+1}/{NUM_CHUNKS}: Series {start_index} to {end_index-1} ---\")\n    \n    current_paths_chunk = batch_series_paths[start_index:end_index]\n    \n    dataset = DicomSeriesDataset(current_paths_chunk, new_spacing=(1, 1, 1), save_dir=OUTPUT_DIR)\n    dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=2)\n    \n    for _, series_name in tqdm(dataloader, desc=f\"Chunk {i+1}\"):\n        pass\n        \n    print(f\"\\nâœ… Chunk {i+1} processed successfully.\")\n    print(f\"All {len(current_paths_chunk)} NIfTI files are saved in: {OUTPUT_DIR}\")\n    \n    zip_filename = f\"chunk_{i+1}_series_{start_index}_to_{end_index-1}.zip\"\n    shutil.make_archive(zip_filename.replace('.zip', ''), 'zip', OUTPUT_DIR)\n    \n    print(\"\\n--- ACTION REQUIRED ---\")\n    print(f\"1. A zip file named '{zip_filename}' has been created in /kaggle/working/.\")\n    print(\"2. Download this zip file to your local machine now.\")\n    print(\"3. After the download is complete, return to this notebook.\")\n\n    if i < NUM_CHUNKS - 1:\n        input(\"4. Press [Enter] here to DELETE the processed files and continue to the next chunk...\")\n        \n        try:\n            shutil.rmtree(OUTPUT_DIR)\n            os.remove(zip_filename)\n            print(f\"ðŸ§¹ Cleaned up {OUTPUT_DIR} and {zip_filename}. Ready for the next chunk.\")\n        except OSError as e:\n            print(f\"Error during cleanup: {e}. Please manually delete the folder.\")\n    else:\n        print(\"\\nðŸŽ‰ All chunks have been processed!\")\n\nprint(\"-\" * 50)\nprint(\"Processing complete.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-31T04:59:58.899002Z","iopub.execute_input":"2025-08-31T04:59:58.899194Z","execution_failed":"2025-08-31T09:33:54.138Z"}},"outputs":[{"name":"stdout","text":"Scanning for series directories in: /kaggle/input/rsna-intracranial-aneurysm-detection/series...\nFound 4348 unique series to process.\nTotal unique series to process: 4348\nProcessing in 44 chunks of up to 100 series each.\n--------------------------------------------------\n\n--- Processing Chunk 1/44: Series 0 to 99 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Chunk 1:   0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ca50460f8164303820e1579d580af73"}},"metadata":{}},{"name":"stderr","text":"WARNING: In /tmp/SimpleITK-build/ITK-prefix/include/ITK-5.4/itkImageSeriesReader.hxx, line 478\nImageSeriesReader (0x404fc350): Non uniform sampling or missing slices detected,  maximum nonuniformity:9.92447e-05\n\nWARNING: In /tmp/SimpleITK-build/ITK-prefix/include/ITK-5.4/itkImageSeriesReader.hxx, line 478\nImageSeriesReader (0x404fc350): Non uniform sampling or missing slices detected,  maximum nonuniformity:0.697059\n\nWARNING: In /tmp/SimpleITK-build/ITK-prefix/include/ITK-5.4/itkImageSeriesReader.hxx, line 478\nImageSeriesReader (0x404fc350): Non uniform sampling or missing slices detected,  maximum nonuniformity:0.000312229\n\nWARNING: In /tmp/SimpleITK-build/ITK-prefix/include/ITK-5.4/itkImageSeriesReader.hxx, line 478\nImageSeriesReader (0x404fc350): Non uniform sampling or missing slices detected,  maximum nonuniformity:0.00014289\n\nWARNING: In /tmp/SimpleITK-build/ITK-prefix/include/ITK-5.4/itkImageSeriesReader.hxx, line 478\nImageSeriesReader (0x404fc350): Non uniform sampling or missing slices detected,  maximum nonuniformity:0.000746138\n\nWARNING: In /tmp/SimpleITK-build/ITK-prefix/include/ITK-5.4/itkImageSeriesReader.hxx, line 478\nImageSeriesReader (0x404fc350): Non uniform sampling or missing slices detected,  maximum nonuniformity:0.000162543\n\nWARNING: In /tmp/SimpleITK-build/ITK-prefix/include/ITK-5.4/itkImageSeriesReader.hxx, line 478\nImageSeriesReader (0x404fc350): Non uniform sampling or missing slices detected,  maximum nonuniformity:0.000101863\n\nWARNING: In /tmp/SimpleITK-build/ITK-prefix/include/ITK-5.4/itkImageSeriesReader.hxx, line 478\nImageSeriesReader (0x404fc350): Non uniform sampling or missing slices detected,  maximum nonuniformity:9.4387e-05\n\n","output_type":"stream"}],"execution_count":null}]}